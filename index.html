<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
	<meta charset="UTF-8">
	<title>10th Annual Minisymposium on Computational Topology</title>
	<link href="workshop.css" rel="stylesheet" type="text/css">
</head>

<body>
	<div class="main_banner1_bar"> </div>
	<div class="main_banner2_bar" style="height:15px"> </div>

	<section>
		<h1>10th Annual Minisymposium on Computational Topology</h1>
    
		<p class="superset">
			Part of <a target="_blank" href="https://www.inf.fu-berlin.de/inst/ag-ti/socg22/">Computational Geometry Week (CG Week 2022)</a>
		
		</p><p class="venue_and_date">
			June 7 and June 8, 2022 in Berlin, Germany
    </p>	
			
    <p></p>
	</section>
	


	

	<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
		<h2> Overview </h2>
		<div class="main_banner2_bar" style="height:5px"> </div>

		<article>
			<h3> The Workshop </h3>
			<p class="textstyle">
Computational topology encompasses computational geometry, algebraic topology, visual computing, and data science. Topology helps recognizing patterns in data and, therefore, turning data into compressed knowledge. Recently, the branch field of Topological Data Analysis (TDA) has become more and more attractive, entailing both theoretical and applied topics: from descriptor definition to computability, from statistical handling to visualization, classification, and comparison. Also, the synergy between TDA and Machine Learning is gaining attention, with research lines ranging from differentiability of topological features to TDA for explainability. This workshop is intended to bring together researchers working in applied, computational, and quantitative topology and topological combinatorics, also with interests in the fields of visual computing as well as machine learning, and to foster exchange of ideas. 
</p>			
		</article>

	

		
		<article>
			<h3> Confirmed Invited Speakers </h3>
			<p>


<!-- <a href="https://www.math.colostate.edu/~adams/">Henry Adams</a> Colorado State University (USA) <br> -->
<a href="http://lxmi.mi.infn.it/~gbocchi/index.html">Giovanni Bocchi</a> Università degli Studi di Milano (Italy) <br>
<a href="https://people.epfl.ch/stefania.ebli?lang=en">Stefania Ebli</a>  École Polytechnique Fédérale de Lausanne (Switzerland) <br>
<a href="http://www.mustafahajij.com">Mustafa Hajij</a> Santa Clara University (USA) <br>
<a href="http://kurlin.org">Vitaliy Kurlin</a> University of Liverpool (UK) <br>
<a href="https://www.kathrynlindsey.com">Kathryn Lindsey</a> Boston College (USA) <br>
<!-- <a href="https://bastian.rieck.me">Bastian Rieck</a> Institute of AI for Health, Helmholtz Zentrum München (Germany) <br>
 --><!-- <a href="http://www.sci.utah.edu/~beiwang/">Bei Wang</a> Institute University of Utah (USA) <br> -->
<a href="http://yusu.belkin-wang.org">Yusu Wang</a> University of California (USA) <br>

<!-- 
<a href="">speaker</a> affiliation <br>
<a href="">speaker</a> affiliation <br>

 -->



				
			</p>
		</article>
		
		<!-- CHANGE: the time for questions and the starting time in the schedule -->
		<article>
			<h3> Format </h3>
			<p class="textstyle">
			The workshop will span two half days.
Four presentations are programmed for the first half day and two for the second one.
The duration of the presentations is 30 minutes for in-person speakers and 20 minutes for remote speakers.
Each presentation will be followed by a Q&A time of 5 minutes.
			</p>			
		</article>

		<article>
			<h3> Intended Audience </h3>
			<p class="textstyle">
			  Besides people working in the field of computational topology, the workshop is targeted at researchers from computational geometry, data analysis, machine learning, statistics, and visual computing, who want to learn about and discuss recent developments in the area. 
			</p>			
		</article>
		

	</section>


	<!-- CHANGE: speakers 	 -->

	<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
    <h2> Schedule </h2>
		<a id="schedule"></a>
		<div class="main_banner2_bar" style="height:5px"> </div>

		<article>
			<h3> Schedule </h3>
			
			<!-- Begin schedule table -->
			<table class="t01">
				<tbody><tr>
					<td class="tablefoot" colspan="4" ;="">Tuesday, June 7, 2022 </td>
				</tr>
				<tr>
					<th style="width:10%"> Time (CET)</th>
					<th style="width:20%"> Speaker </th>
					<th style="width:10%"> Type  </th> 
					<th style="width:60%"> Title </th>
				</tr>
				<!-- <tr>
				  <td> 1:20pm-2:05pm </td>
					<td>  </td>
					<td><a href="#Speaker1">Rocio Gonzalez Diaz</a> </td>
				</tr> -->
				<tr>
					<td> 4:00pm-4:05pm </td>
					<td>   </td>
					<td>  </td>
					<td> Welcoming </td>
				</tr>
				<tr>
					<td> 4:05pm - 4:40pm </td>
					<td> <a href="#Speaker6">Giovanni Bocchi</a> </td>
					<td> in-person  </td>
					<td>    </td>
				</tr>
				<tr>
					<td> 4:40pm-5:05pm </td>
					<td> <a href="#Speaker4">Kathryn Lindsey</a>  </td>
					<td> remote  </td>
					<td>    </td>
				</tr>
				<tr>
					<td> 5:05pm-5:30pm </td>
					<td>    </td>
					<td>  </td>
					<td> Break </td>
				</tr>
				<tr>
					<td> 5:30m-6:05pm </td>
					<td> <a href="#Speaker3">Vitaliy Kurlin</a> </td>
					<td> in-person  </td>
					<td>    </td>

				</tr>
				<tr>
					<td> 6:05pm - 6:30pm </td>
					<td><a href="#Speaker5">Yusu Wang</a> </td>
					<td> remote  </td>
					<td>    </td>
				</tr>
				
			
				
				<tr>
				<td class="tablefoot" colspan="4" ;=""></td>
				</tr>
				
			

			</tbody></table>

			<table class="t01">
				<tbody><tr>
					<td class="tablefoot" colspan="4" ;=""> Wednesday, June 8, 2022 </td>
				</tr>
				<tr>
					<th style="width:10%"> Time (CET)</th>
					<th style="width:20%"> Speaker </th>
					<th style="width:10%"> Type  </th> 
					<th style="width:60%"> Title </th>
				</tr>
				<tr>
					<td> 3:30pm-3:35pm </td>
					<td>    </td>
					<td>  </td>
					<td> Welcoming </td>
				</tr>
				<tr>
				  <td> 3:35pm-4:10pm </td>
					<td> <a href="#Speaker8">Stefania Ebli</a>  </td>
					<td> in-person  </td>
					<td>  </td>
					
				</tr>
				<tr>
					<td> 4:10pm-4:35pm </td>
					<td> <a href="#Speaker2">Mustafa Hajij</a> </td>
					<td> remote  </td>
					<td>  </td>
				</tr>
				<tr>
					<td> 4:35pm-5:00pm </td>
					<td>   </td>
					<td>  </td>
					<td> Discussion and closing </td>
				</tr>
			


				
			
				<tr>
				<td class="tablefoot" colspan="4" ;=""></td>
				</tr>
			</tbody></table>


		</article>
	</section>

	<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
    <h2> Abstracts </h2>
		<div class="main_banner2_bar" style="height:5px"> </div>

		<!-- <a id="Speaker1"></a>
		<div class="abstract_block">
		  <p class="abstract_title">
		    TITLE

			</p>
			<p class="abstract_author">
			Rocio Gonzalez Diaz 
			</p>
			<p class="abstract">
			  	ABSTRACT
			  	<br><br>This is joint work with

			 <a href="NoteJun82021.pdf">Slides</a>
			</p>
		</div> -->
		
		<a id="Speaker2"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			Higher-Order Attention Networks
			</p>
			<p class="abstract_author">
			Mustafa Hajij 
			</p>
			<p class="abstract">
			Over the past decade, deep learning  has been remarkably successful at solving a massive set of problems on data types including images and sequential data.  This success drove the  extension  of  deep  learning  to  other  discrete  domains  such  as  sets,  point  clouds, graphs ,3D  shapes,  and  discrete  manifolds.   While  many  of  the extended schemes have successfully tackled notable challenges in each particular domain, the plethora of  fragmented  frameworks  have  created  or  resurfaced  many  long-standing  problems  in  deep  learning such  as  explainability,  expressiveness  and  generalizability.  Moreover,  theoretical  development  proven over one discrete domain does not naturally apply to the other domains. Finally, the lack of a cohesive mathematical framework has created many ad hoc and inorganic implementations and ultimately limited the set of practitioners that can potentially benefit from deep learning technologies. In this talk I will talk about a generalized higher-order domain called combinatorial complex (CC) and utilize it to build a new class of attention-based neural networks called higher-order attention networks (HOANs). CCs generalize many discrete domains that are of practical importance such as point clouds, 3D shapes, (hyper)graphs, simplicial complexes, and cell complexes. The topological structure of a CC encodes arbitrary higher-order interactions among elements of the CC. By exploiting the rich combinatorial and topological structure of CCs, HOANs define a new class of higher-order message passing attention-based networks that unify existing higher-order models based on hypergraphs and cell complexes. I will demonstrate the reducibility of any CC to a special graph called the Hasse graph, which allows the characterization of certain aspects of HOANs and other higher order models in terms of graph-based models. Finally the predictive capacity of HOANs will be demonstrated in shape analysis and in graph learning, competing against state-of-the-art task-specific neural networks.

			<!-- <a href="NoteJun82021.pdf">Slides</a> -->
			</p>
		</div>


 
		<a id="Speaker3"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			Persistence vs newer isometry invariants of point sets
			</p>
			<p class="abstract_author">
			Vitaliy Kurlin 
			</p>
			<p class="abstract">	 
			For standard filtrations (of Vietoris-Rips, Cech, Delaunay
			complexes) on a finite set of points A in R^N, persistent homology is
			an isometry invariant of A, preserved by all transformations that
			maintain the inter-point distances. The stability theorem gives an
			upper bound on the bottleneck distance between persistence diagrams
			under bounded noise. There is no lower bound because any set A can be
			extended to a large family of non-isometric sets `A union T' that have
			the same 1-dimensional persistence as A, see <a href="https://arxiv.org/abs/2202.00577"> arxiv:2202.00577</a>, by
			adding a `tail' T of points to A. All finite sets of points in general
			position are distinguished up to isometry by the new continuous
			invariant PDD (Pointwise Distance Distribution). For a point p in A,
			the PDD matrix has the row of ordered distances to k neighbors of p,
			see <a href="https://arxiv.org/abs/2108.04798"> arxiv:2108.04798</a>. This isometry invariant is generically complete
			for finite and periodic point sets and can be computed in a
			near-linear time in the number of points by using a fast k-nearest
			neighbor search. The 200B+ comparisons of 660K+ crystals over a couple
			of days on a modest desktop identified five duplicates in the world's
			largest Cambridge Structural Database, which were missed by all past
			tools, now under investigation by five journals for data integrity.
			Hence the new isometry invariants are simpler, faster, and stronger
			than persistence.
			<!-- <br><br>This is joint work with -->
			</p>
		</div>

		<a id="Speaker4"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			Feedforward ReLU neural networks and piecewise-linear geometry
			</p>
			<p class="abstract_author">
			Kathryn Lindsey
			</p>
			<p class="abstract">
Feedforward ReLU neural networks are a relatively simple type of neural network used in machine learning.  It is well known that the class of functions realized by feedforward ReLU neural networks coincides with the class of finitely piecewise-linear (PL) functions.  However, the finer question of classifying which finitely PL functions can be realized by which network architecture -- in both theory and practice -- remains mysterious.  Thus, it is natural to investigate various measures of complexity for functions of this type, and to seek bounds on these complexity measures in terms of the network architecture.  In particular, the topological complexity of sublevel sets of these functions is of interest.  I will discuss recent joint work with E. Grigsby and M. Masden in which we apply ideas from PL Morse theory to describe the complexity of feedforward ReLU neural network functions. 
			<!-- <br><br>This is joint work with -->
			</p>
		</div>


<!-- 		<div class="schedulelink">
			<a href="#schedule">Back to Schedule</a>
		</div>
 -->
		<a id="Speaker5"></a>
		<div class="abstract_block">
			<p class="abstract_title">
				Persistent Laplacian: properties and algorithms
			</p>
			<p class="abstract_author">
				Yusu Wang
			</p>
			<p class="abstract">
			The combinatorial graph Laplacian, as an operator on functions defined on the vertex set of a graph, is a fundamental object in the analysis of and optimization on graphs. There is also an algebraic topology view of the graph Laplacian which arises through considering boundary operators and specific inner products defined on simplicial (co)chain groups. This permits extending the graph Laplacian to a more general operator, the q-th combinatorial Laplacian to a given simplicial complex. An extension of this combinatorial Laplacian to the setting of pairs (or more generally, a sequence of) simplicial complexes was recently introduced by (R.) Wang, Nguyen and Wei. In this talk, I will present several results (including a persistent version of the Cheeger inequality) from our recent study of the theoretical properties for the persistence Laplacian, as well as efficient algorithms to compute it. This is joint work with Facundo Memoli and Zhengchao Wan. 
			<!-- <br><br>This is joint work with -->
			</p>
		</div>


		<a id="Speaker6"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			Group Equivariant Non-Expansive Operators: a novel approach to deep learning grounded in Topological Data Analysis
			</p>
			<p class="abstract_author">
			 Giovanni Bocchi 
			</p>
			<p class="abstract">
			Equivariance is now acknowledged as an essential property when working with data that can be transformed by mean of geometrical transformations. In accordance with this, Group Equivariant Non-Expansive Operators (GENEOs) are recently emerging as new tools to build different types of neural networks both explainable and capable of exploiting prior knowledge. In such models standard neurons are replaced by families of operators whose equivariance is set with respect to specific groups of transformations. This is probably the most challenging topic of the research on GENEOs, but, in addition to this, there also other topics that establish deep connections between GENEOs and Topological Data Analysis (TDA). One of the most relevant is that GENEOs can be employed to restrict the intrinsic invariance of TDA, which is insensitive to homeomorphisms, only to proper subgroups of homeomorphisms. In many cases invariance with respect to all homeomorphisms is definitely too large, thus GENEOs allow to analyze data  focusing only on some geometrical characteristics. 
First of all GENEOs will be presented together with the properties that make them interesting from a deep learning perspective, with some links to TDA. Lastly some of the possible applications of GENEOs to different case studies will be reviewed.

			<!-- <br><br>This is joint work with -->
			</p>
		</div>
		

		<!-- <a id="Speaker7"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			TITLE
			</p>
			<p class="abstract_author">
			Catalina Cangea 
			</p>
			<p class="abstract">
			  ABSTRACT
			  <br><br>This is joint work with
			</p>
		</div> -->

		<a id="Speaker8"></a>
		<div class="abstract_block">
			<p class="abstract_title">
			Morse-theoretic signal compression and reconstruction 
			</p>
			<p class="abstract_author">
			Stefania Ebli
			</p>
			<p class="abstract">
			In this talk I will present novel results at the intersection of cellular signal processing and discrete Morse theory. In the usual paradigm, the signals on a simplicial or chain complex are processed using the combinatorial Laplacian and the resultant Hodge decomposition. On the other hand, discrete Morse theory has been widely used to speed up computations, by reducing the size of complexes while preserving their global topological properties.
Our approach to signal compression and reconstruction on chain complexes leverages the tools of algebraic discrete Morse theory, which provides a method to reduce and reconstruct a based chain complex together with a set of signals on its cells via deformation retracts, preserving as much as possible the global topological structure of both the complex and the signals. It turns out that any deformation retract of real degreewise finite-dimensional based chain complexes is equivalent to a Morse matching. Moreover, in the case of a particular type of Morse matchings, the reconstruction error is trivial, except on one specific component of the Hodge decomposition. Finally, we developed and implemented an algorithm to compute Morse matchings with minimal reconstruction error, of which I will show explicit examples.
This is joint work with Celia Hacker and Kelly Maggs
			</p>
		</div>

		<div class="schedulelink">
			<a href="#schedule">Back to Schedule</a>
		</div>

	</section>

	

	<div class="main_banner2_bar" style="height:15px"> </div>
	<div class="main_banner1_bar"> </div>


	  
	  	<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
    <h2> Organizers </h2>
		<div class="main_banner2_bar" style="height:5px"> </div>
		
<p class="organizer">
			<a target="_blank" href="http://personale.unimore.it/rubrica/dettaglio/clandi">Claudia Landi</a><br>
			University of Modena and Reggio Emilia, Italy <br>
			claudia.landiATunimore.it<br>
</p>
		

		
<p class="organizer">
			<a target="_blank" href="http://vcg.isti.cnr.it/~giorgi/">Daniela Giorgi</a><br>
			 Visual Computing Lab, National Research Council of Italy <br>
			daniela.giorgiATisti.cnr.it<br>
</p>


<p class="organizer">
			<a target="_blank" href="https://fugacci.github.io/home/">Ulderico Fugacci</a><br>
			Institute for Applied Mathematics and Information Technologies, National Research Council of Italy<br>
			ulderico.fugacciATimati.cnr.it<br>
</p>

<p class="organizer">
			<a target="_blank" href="https://sarascaramuccia.wixsite.com/scara">Sara Scaramuccia</a><br>
			University of Rome Tor Vergata, Italy<br>
			sara.scaramucciaATuniroma2.it<br>
</p>

<!-- 		</section>
		<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
    <h2>Support</h2>
    <div class="main_banner2_bar" style="height:5px"> </div>			  
			<p>
We gratefully acknowledge support ...
 -->
<!--  of the <a href="http://ligm.u-pem.fr/">Laboratoire d'Informatique Gaspard Monge</a> and the <a href="https://www.discretization.de/">Discretization in Geometry and Dynamics</a> project.
 -->
<!-- </section> -->


	<section>
		<div class="main_banner2_bar" style="height:5px"> </div>
    <h2>Past iterations</h2>
    <div class="main_banner2_bar" style="height:5px"> </div>

    		<p>

		  <a href="http://monge.univ-mlv.fr/~demesma/Workshop.html">9th Annual Minisymposium on Computational Topology (Buffalo)</a> <br>
		  <a href="http://www.sci.utah.edu/~beiwang/socgtda2019/">8th Annual Minisymposium on Computational Topology (Portland)</a> <br>
		    <a href="https://sites.google.com/view/socg-comptop-2018">7th Annual Minisymposium on Computational Topology (Budapest)</a> <br>
		      <a href=" http://www.wpi-aimr.tohoku.ac.jp/hiraoka_labo/eescolar/comptopo2017/">6th Annual Minisymposium on Computational Topology (Brisbane)</a> <br>
			5th Annual Minisymposium on Computational Topology (Boston)<br>
		      <a href="https://www.win.tue.nl/SoCG2015/Computational_Topology">4th Annual Minisymposium on Computational Topology (Eindhoven)</a> <br>
		      <a href="https://web.archive.org/web/20130910213353/http://zeus.mat.puc-rio.br/~socg2013/index.php/workshops/computational-topology-and-data-analysis">Workshop on Computational Topology and Data Analysis (Rio)</a><br>
		      <a href="https://mrzv.org/meetings/socg12-comptop/">Workshop on Computational Topology (Chapel Hill) </a><br>
		</section>	  


    
</body></html>

